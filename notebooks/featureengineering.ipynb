{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7fae47",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b976ee0",
   "metadata": {},
   "source": [
    "## Definition and a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814d5eb",
   "metadata": {},
   "source": [
    "Feature engineering - the process of transforming raw data into meaningful input features that better represent the underlying problem, improving the performance and accuracy of machine learning models. This critical data science technique involves selecting, creating, and transforming variables to enhance the data's predictive power and make it more suitable for algorithms to learn from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8519009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2043f4f",
   "metadata": {},
   "source": [
    "First of all, let's read up the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affe9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\2187268715.py:1: DtypeWarning: Columns (14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train=pd.read_csv('final_dataframe.csv')\n",
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\2187268715.py:2: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test=pd.read_csv('final_dataframe_test.csv')\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('final_dataframe.csv')\n",
    "test=pd.read_csv('final_dataframe_test.csv')\n",
    "final_test=pd.read_csv('final_future_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa563a3",
   "metadata": {},
   "source": [
    "It took more than 10 minutes to read all the dataframes. It would be easier if I reduce the memory of all of those by converting all categorical variables to integer. Also, we save here label encoders data so we can use them to encode our future unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4590d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['item_id']=lbl.fit_transform(train['item_id'])\n",
    "test['item_id']=lbl.transform(test['item_id'])\n",
    "final_test['item_id']=lbl.transform(final_test['item_id'])\n",
    "pickle.dump(lbl,open('label_encoder_item_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcd916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['dept_id']=lbl.fit_transform(train['dept_id'])\n",
    "test['dept_id']=lbl.transform(test['dept_id'])\n",
    "final_test['dept_id']=lbl.transform(final_test['dept_id'])\n",
    "pickle.dump(lbl,open('label_encoder_dept_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e51df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['cat_id']=lbl.fit_transform(train['cat_id'])\n",
    "test['cat_id']=lbl.transform(test['cat_id'])\n",
    "final_test['cat_id']=lbl.transform(final_test['cat_id'])\n",
    "pickle.dump(lbl,open('label_encoder_cat_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4cbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['store_id']=lbl.fit_transform(train['store_id'])\n",
    "test['store_id']=lbl.transform(test['store_id'])\n",
    "final_test['store_id']=lbl.transform(final_test['store_id'])\n",
    "pickle.dump(lbl,open('label_encoder_store_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7beb2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['state_id']=lbl.fit_transform(train['state_id'])\n",
    "test['state_id']=lbl.transform(test['state_id'])\n",
    "final_test['state_id']=lbl.transform(final_test['state_id'])\n",
    "pickle.dump(lbl,open('label_encoder_state_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4febd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_name_1 encoding\n",
    "train['event_name_1'] = train['event_name_1'].fillna('no_event')\n",
    "test['event_name_1'] = test['event_name_1'].fillna('no_event')\n",
    "final_test['event_name_1'] = final_test['event_name_1'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_name_1'] = train['event_name_1'].astype(str)\n",
    "test['event_name_1'] = test['event_name_1'].astype(str)\n",
    "final_test['event_name_1'] = final_test['event_name_1'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_name_1'].values,\n",
    "    test['event_name_1'].values,\n",
    "    final_test['event_name_1'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_name_1'] = lbl.transform(train['event_name_1'])\n",
    "test['event_name_1'] = lbl.transform(test['event_name_1'])\n",
    "final_test['event_name_1'] = lbl.transform(final_test['event_name_1'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_name_1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a6c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_name_2 encoding\n",
    "train['event_name_2'] = train['event_name_2'].fillna('no_event')\n",
    "test['event_name_2'] = test['event_name_2'].fillna('no_event')\n",
    "final_test['event_name_2'] = final_test['event_name_2'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_name_2'] = train['event_name_2'].astype(str)\n",
    "test['event_name_2'] = test['event_name_2'].astype(str)\n",
    "final_test['event_name_2'] = final_test['event_name_2'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_name_2'].values,\n",
    "    test['event_name_2'].values,\n",
    "    final_test['event_name_2'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_name_2'] = lbl.transform(train['event_name_2'])\n",
    "test['event_name_2'] = lbl.transform(test['event_name_2'])\n",
    "final_test['event_name_2'] = lbl.transform(final_test['event_name_2'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_name_2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94410721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_type_1 encoding\n",
    "train['event_type_1'] = train['event_type_1'].fillna('no_event')\n",
    "test['event_type_1'] = test['event_type_1'].fillna('no_event')\n",
    "final_test['event_type_1'] = final_test['event_type_1'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_type_1'] = train['event_type_1'].astype(str)\n",
    "test['event_type_1'] = test['event_type_1'].astype(str)\n",
    "final_test['event_type_1'] = final_test['event_type_1'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_type_1'].values,\n",
    "    test['event_type_1'].values,\n",
    "    final_test['event_type_1'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_type_1'] = lbl.transform(train['event_type_1'])\n",
    "test['event_type_1'] = lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1'] = lbl.transform(final_test['event_type_1'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_type_1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f08ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_type_2 encoding\n",
    "train['event_type_2'] = train['event_type_2'].fillna('no_event')\n",
    "test['event_type_2'] = test['event_type_2'].fillna('no_event')\n",
    "final_test['event_type_2'] = final_test['event_type_2'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_type_2'] = train['event_type_2'].astype(str)\n",
    "test['event_type_2'] = test['event_type_2'].astype(str)\n",
    "final_test['event_type_2'] = final_test['event_type_2'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_type_2'].values,\n",
    "    test['event_type_2'].values,\n",
    "    final_test['event_type_2'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_type_2'] = lbl.transform(train['event_type_2'])\n",
    "test['event_type_2'] = lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2'] = lbl.transform(final_test['event_type_2'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_type_2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a423f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_1']=train['event_type_1'].fillna('no_event')\n",
    "test['event_type_1']=test['event_type_1'].fillna('no_event')\n",
    "final_test['event_type_1']=final_test['event_type_1'].fillna('no_event')\n",
    "train['event_type_1']=lbl.fit_transform(train['event_type_1'])\n",
    "test['event_type_1']=lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1']=lbl.transform(final_test['event_type_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f51351",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_2']=train['event_type_2'].fillna('no_event')\n",
    "test['event_type_2']=test['event_type_2'].fillna('no_event')\n",
    "final_test['event_type_2']=final_test['event_type_2'].fillna('no_event')\n",
    "train['event_type_2']=lbl.fit_transform(train['event_type_2'])\n",
    "test['event_type_2']=lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2']=lbl.transform(final_test['event_type_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ba4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['year']=lbl.fit_transform(train['year'])\n",
    "test['year']=lbl.transform(test['year'])\n",
    "final_test['year']=lbl.transform(final_test['year'])\n",
    "pickle.dump(lbl,open('label_encoder_year.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954ffcc",
   "metadata": {},
   "source": [
    "After the data reducing has been done, we can remove unnecessary columns. Firstly, let's convert all 3 state SNAPs into one feature named SNAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de4f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 56s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.loc[train['state_id'] == 'CA', 'snap'] = train.loc[train['state_id'] == 'CA']['snap_CA']\n",
    "train.loc[train['state_id'] == 'TX', 'snap'] = train.loc[train['state_id'] == 'TX']['snap_TX']\n",
    "train.loc[train['state_id'] == 'WI', 'snap'] = train.loc[train['state_id'] == 'WI']['snap_WI']\n",
    "train.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "test.loc[test['state_id'] == 'CA', 'snap'] = test.loc[test['state_id'] == 'CA']['snap_CA']\n",
    "test.loc[test['state_id'] == 'TX', 'snap'] = test.loc[test['state_id'] == 'TX']['snap_TX']\n",
    "test.loc[test['state_id'] == 'WI', 'snap'] = test.loc[test['state_id'] == 'WI']['snap_WI']\n",
    "test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "final_test.loc[final_test['state_id'] == 'CA', 'snap'] = final_test.loc[final_test['state_id'] == 'CA']['snap_CA']\n",
    "final_test.loc[final_test['state_id'] == 'TX', 'snap'] = final_test.loc[final_test['state_id'] == 'TX']['snap_TX']\n",
    "final_test.loc[final_test['state_id'] == 'WI', 'snap'] = final_test.loc[final_test['state_id'] == 'WI']['snap_WI']\n",
    "final_test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f904d5e",
   "metadata": {},
   "source": [
    "Weekday = wday are similar features so there is no need to keep it. The same reason for having wm_yr_wk feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe5713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 44s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.drop('weekday',axis=1,inplace=True)\n",
    "train.drop('wm_yr_wk',axis=1,inplace=True)\n",
    " \n",
    "test.drop('weekday',axis=1,inplace=True)\n",
    "test.drop('wm_yr_wk',axis=1,inplace=True)\n",
    "\n",
    "final_test.drop('weekday',axis=1,inplace=True)\n",
    "final_test.drop('wm_yr_wk',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fc87d",
   "metadata": {},
   "source": [
    "FEATURES THAT INCLUDE TIME INTERVALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d35b4a",
   "metadata": {},
   "source": [
    "a) Number of the week - I created the function to get the week number of particular date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460cde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_number(x):\n",
    "    date=calendar.datetime.date.fromisoformat(x)\n",
    "    return date.isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a85b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['week_number']=train['date'].apply(lambda x:get_week_number(x))\n",
    "test['week_number']=test['date'].apply(lambda x:get_week_number(x))\n",
    "final_test['week_number']=final_test['date'].apply(lambda x:get_week_number(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e3e1d",
   "metadata": {},
   "source": [
    "b) Season of the year - A function that is used to get season according to the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22a7532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(x):\n",
    "    if x in [12,1,2]:\n",
    "        return 0      #\"Winter\"\n",
    "    elif x in [3,4,5]:\n",
    "        return 1   #\"Spring\"\n",
    "    elif x in [6,7,8]:\n",
    "        return 2   #\"Summer\"\n",
    "    else:\n",
    "        return 3   #\"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d28817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['season']=train['month'].apply(lambda x:get_season(x))\n",
    "test['season']=test['month'].apply(lambda x:get_season(x))\n",
    "final_test['season']=final_test['month'].apply(lambda x:get_season(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f89bf5",
   "metadata": {},
   "source": [
    "c) Start of a quarter - A function used to check which day starts the quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76a4a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quarter_begin(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and (month in [1,4,7,9])) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e6264cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quarter_start']=train['date'].apply(lambda x:check_if_quarter_begin(x))\n",
    "test['quarter_start']=test['date'].apply(lambda x:check_if_quarter_begin(x))\n",
    "final_test['quarter_start']=final_test['date'].apply(lambda x:check_if_quarter_begin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d75b4d",
   "metadata": {},
   "source": [
    "d) End of a quarter - A function used to check which day ends the quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61e2d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quarter_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    if (day==31 and month==3) or (day==30 and month==6) or (day==30 and month==9) or (day==31 and month==12):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ed262f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quarter_end']=train['date'].apply(lambda x:check_if_quarter_end(x))\n",
    "test['quarter_end']=test['date'].apply(lambda x:check_if_quarter_end(x))\n",
    "final_test['quarter_end']=final_test['date'].apply(lambda x:check_if_quarter_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e38e6",
   "metadata": {},
   "source": [
    "e) Start of a month - The function below checks if the day is beginning of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b4d6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def month_start(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    return 1 if day==1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05881d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_start']=train['date'].apply(lambda x:month_start(x))\n",
    "test['month_start']=test['date'].apply(lambda x:month_start(x))\n",
    "final_test['month_start']=final_test['date'].apply(lambda x:month_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d5ed9",
   "metadata": {},
   "source": [
    "f) End of a month - The function below checks if the day is end of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c31f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    year=calendar.datetime.date.fromisoformat(x).year\n",
    "    leap_yr=(year%4==0) # Checking if it is a leap year\n",
    "    val=(day==31 and month==1) or (day==29 if leap_yr else day==28) or (day==31 and month==3) or (day==30 and month==4) or\\\n",
    "        (day==31 and month==5) or (day==30 and month==6) or (day==31 and month==7) or (day==31 and month==8) or\\\n",
    "        (day==30 and month==9) or (day==31 and month==10) or (day==30 and month==11) or (day==31 and month==12)\n",
    "    return 1 if val else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d54e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['month_end']=train['date'].apply(lambda x:month_end(x))\n",
    "test['month_end']=test['date'].apply(lambda x:month_end(x))\n",
    "final_test['month_end']=final_test['date'].apply(lambda x:month_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaf4b8",
   "metadata": {},
   "source": [
    "g) Start of a year - The function checking if a given day is the beginning of a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c808667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_start(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and month==1) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3bb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_start']=train['date'].apply(lambda x:year_start(x))\n",
    "test['year_start']=test['date'].apply(lambda x:year_start(x))\n",
    "final_test['year_start']=final_test['date'].apply(lambda x:year_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bf558",
   "metadata": {},
   "source": [
    "h) End of a year - The function checking if a given day is the end of a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6197d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==31 and month==12) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb2e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_end']=train['date'].apply(lambda x:year_end(x))\n",
    "test['year_end']=test['date'].apply(lambda x:year_end(x))\n",
    "final_test['year_end']=final_test['date'].apply(lambda x:year_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cf509",
   "metadata": {},
   "source": [
    "We can take the last 28 days from the train data for cross validation that could be used for further modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5bb4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=train[train['date']>='2016-03-28']\n",
    "train=train[train['date']<'2016-03-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc779589",
   "metadata": {},
   "source": [
    "Last but not least, I will create a time series related features. I am going to create direct feature to test and train data. Below I wrote a code that creates a large data for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f398c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 55s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "tt=pd.concat([train,cv,test,final_test])\n",
    "tt.sort_values(['id','date'],inplace=True)\n",
    "df=tt.pivot_table(index=['item_id','store_id'],columns='date',values='sales')\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a74c37",
   "metadata": {},
   "source": [
    "My next step that I will take will be the calculation of a rolling mean and standard deviation. I also took 28 days based on requirements and to avoid the data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d500500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_7_shift_28_mean...\n",
      "Merging roll_7_shift_28_mean into train...\n",
      "Merging roll_7_shift_28_mean into train...\n",
      "Merging roll_7_shift_28_mean into cv...\n",
      "Merging roll_7_shift_28_mean into cv...\n",
      "Merging roll_7_shift_28_mean into test...\n",
      "Merging roll_7_shift_28_mean into test...\n",
      "Merging roll_7_shift_28_mean into final_test...\n",
      "Merging roll_7_shift_28_mean into final_test...\n",
      "Successfully added feature roll_7_shift_28_mean\n",
      "Successfully added feature roll_7_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_14_shift_28_mean...\n",
      "Combining features for roll_14_shift_28_mean...\n",
      "Merging roll_14_shift_28_mean into train...\n",
      "Merging roll_14_shift_28_mean into train...\n",
      "Merging roll_14_shift_28_mean into cv...\n",
      "Merging roll_14_shift_28_mean into cv...\n",
      "Merging roll_14_shift_28_mean into test...\n",
      "Merging roll_14_shift_28_mean into test...\n",
      "Merging roll_14_shift_28_mean into final_test...\n",
      "Merging roll_14_shift_28_mean into final_test...\n",
      "Successfully added feature roll_14_shift_28_mean\n",
      "Successfully added feature roll_14_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_30_shift_28_mean...\n",
      "Combining features for roll_30_shift_28_mean...\n",
      "Merging roll_30_shift_28_mean into train...\n",
      "Merging roll_30_shift_28_mean into train...\n",
      "Merging roll_30_shift_28_mean into cv...\n",
      "Merging roll_30_shift_28_mean into cv...\n",
      "Merging roll_30_shift_28_mean into test...\n",
      "Merging roll_30_shift_28_mean into test...\n",
      "Merging roll_30_shift_28_mean into final_test...\n",
      "Merging roll_30_shift_28_mean into final_test...\n",
      "Successfully added feature roll_30_shift_28_mean\n",
      "Successfully added feature roll_30_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_60_shift_28_mean...\n",
      "Combining features for roll_60_shift_28_mean...\n",
      "Merging roll_60_shift_28_mean into train...\n",
      "Merging roll_60_shift_28_mean into train...\n",
      "Merging roll_60_shift_28_mean into cv...\n",
      "Merging roll_60_shift_28_mean into cv...\n",
      "Merging roll_60_shift_28_mean into test...\n",
      "Merging roll_60_shift_28_mean into test...\n",
      "Merging roll_60_shift_28_mean into final_test...\n",
      "Merging roll_60_shift_28_mean into final_test...\n",
      "Successfully added feature roll_60_shift_28_mean\n",
      "Successfully added feature roll_60_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_360_shift_28_mean...\n",
      "Combining features for roll_360_shift_28_mean...\n",
      "Merging roll_360_shift_28_mean into train...\n",
      "Merging roll_360_shift_28_mean into train...\n",
      "Merging roll_360_shift_28_mean into cv...\n",
      "Merging roll_360_shift_28_mean into cv...\n",
      "Merging roll_360_shift_28_mean into test...\n",
      "Merging roll_360_shift_28_mean into test...\n",
      "Merging roll_360_shift_28_mean into final_test...\n",
      "Merging roll_360_shift_28_mean into final_test...\n",
      "Successfully added feature roll_360_shift_28_mean\n",
      "Successfully added feature roll_360_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_7_shift_28_std...\n",
      "Combining features for roll_7_shift_28_std...\n",
      "Merging roll_7_shift_28_std into train...\n",
      "Merging roll_7_shift_28_std into train...\n",
      "Merging roll_7_shift_28_std into cv...\n",
      "Merging roll_7_shift_28_std into cv...\n",
      "Merging roll_7_shift_28_std into test...\n",
      "Merging roll_7_shift_28_std into test...\n",
      "Merging roll_7_shift_28_std into final_test...\n",
      "Merging roll_7_shift_28_std into final_test...\n",
      "Successfully added feature roll_7_shift_28_std\n",
      "Successfully added feature roll_7_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_14_shift_28_std...\n",
      "Combining features for roll_14_shift_28_std...\n",
      "Merging roll_14_shift_28_std into train...\n",
      "Merging roll_14_shift_28_std into train...\n",
      "Merging roll_14_shift_28_std into cv...\n",
      "Merging roll_14_shift_28_std into cv...\n",
      "Merging roll_14_shift_28_std into test...\n",
      "Merging roll_14_shift_28_std into test...\n",
      "Merging roll_14_shift_28_std into final_test...\n",
      "Merging roll_14_shift_28_std into final_test...\n",
      "Successfully added feature roll_14_shift_28_std\n",
      "Successfully added feature roll_14_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_30_shift_28_std...\n",
      "Merging roll_30_shift_28_std into train...\n",
      "Merging roll_30_shift_28_std into train...\n",
      "Merging roll_30_shift_28_std into cv...\n",
      "Merging roll_30_shift_28_std into cv...\n",
      "Merging roll_30_shift_28_std into test...\n",
      "Merging roll_30_shift_28_std into test...\n",
      "Merging roll_30_shift_28_std into final_test...\n",
      "Merging roll_30_shift_28_std into final_test...\n",
      "Successfully added feature roll_30_shift_28_std\n",
      "Successfully added feature roll_30_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_60_shift_28_std...\n",
      "Merging roll_60_shift_28_std into train...\n",
      "Merging roll_60_shift_28_std into train...\n",
      "Merging roll_60_shift_28_std into cv...\n",
      "Merging roll_60_shift_28_std into cv...\n",
      "Merging roll_60_shift_28_std into test...\n",
      "Merging roll_60_shift_28_std into test...\n",
      "Merging roll_60_shift_28_std into final_test...\n",
      "Merging roll_60_shift_28_std into final_test...\n",
      "Successfully added feature roll_60_shift_28_std\n",
      "Successfully added feature roll_60_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_360_shift_28_std...\n",
      "Combining features for roll_360_shift_28_std...\n",
      "Merging roll_360_shift_28_std into train...\n",
      "Merging roll_360_shift_28_std into train...\n",
      "Merging roll_360_shift_28_std into cv...\n",
      "Merging roll_360_shift_28_std into cv...\n",
      "Merging roll_360_shift_28_std into test...\n",
      "Merging roll_360_shift_28_std into test...\n",
      "Merging roll_360_shift_28_std into final_test...\n",
      "Merging roll_360_shift_28_std into final_test...\n",
      "Successfully added feature roll_360_shift_28_std\n",
      "CPU times: total: 6min 57s\n",
      "Wall time: 7min\n",
      "Successfully added feature roll_360_shift_28_std\n",
      "CPU times: total: 6min 57s\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reduce df memory before heavy operations\n",
    "if df.values.dtype != 'float16':  # Use float16 instead of float32 to reduce memory\n",
    "    df = df.astype('float16')\n",
    "\n",
    "date_cols = list(df.columns)\n",
    "ncols = len(date_cols)\n",
    "chunk_size = 500  # Reduced chunk size to prevent memory issues\n",
    "\n",
    "# Track created features to avoid duplicates\n",
    "created_features = set()\n",
    "\n",
    "# Function to safely merge features\n",
    "def safe_merge(df, features, name):\n",
    "    # Drop the feature column if it already exists to avoid conflicts\n",
    "    if name in df.columns:\n",
    "        df = df.drop(columns=[name])\n",
    "    return df.merge(features, on=['item_id', 'store_id', 'date'], how='left')\n",
    "\n",
    "for aggregate in ['mean', 'std']:\n",
    "    for shif in [28]:\n",
    "        for r in [7, 14, 30, 60, 360]:\n",
    "            name = f\"roll_{r}_shift_{shif}_{aggregate}\"\n",
    "            if name in created_features:\n",
    "                print(f\"Skipping {name} - already created\")\n",
    "                continue\n",
    "                \n",
    "            pad = r - 1\n",
    "            feature_created = False\n",
    "            all_features = []\n",
    "            \n",
    "            for start in range(0, ncols, chunk_size):\n",
    "                # Clear memory at start of each iteration\n",
    "                gc.collect()\n",
    "                \n",
    "                left = max(0, start - pad)\n",
    "                right = min(ncols, start + chunk_size)\n",
    "                keep_start = start\n",
    "                keep_end = min(start + chunk_size, ncols)\n",
    "\n",
    "                try:\n",
    "                    # Get subset of columns including padding\n",
    "                    sub_cols = date_cols[left:right]\n",
    "                    sub_df = df.loc[:, sub_cols].copy()  # Make an explicit copy\n",
    "\n",
    "                    # Compute rolling stats\n",
    "                    roll = sub_df.rolling(r, axis=1).agg(aggregate).shift(shif, axis=1)\n",
    "                    \n",
    "                    # Keep only the needed columns\n",
    "                    keep_cols = date_cols[keep_start:keep_end]\n",
    "                    roll_sel = roll.loc[:, [c for c in keep_cols if c in roll.columns]]\n",
    "                    \n",
    "                    if roll_sel.shape[1] == 0:\n",
    "                        del sub_df, roll, roll_sel\n",
    "                        continue\n",
    "\n",
    "                    # Process in smaller batches for melting\n",
    "                    batch_size = 50000  # Reduced batch size\n",
    "                    n_batches = (len(roll_sel) + batch_size - 1) // batch_size\n",
    "                    \n",
    "                    for b in range(n_batches):\n",
    "                        start_idx = b * batch_size\n",
    "                        end_idx = min((b + 1) * batch_size, len(roll_sel))\n",
    "                        \n",
    "                        # Process batch\n",
    "                        roll_batch = roll_sel.iloc[start_idx:end_idx].reset_index()\n",
    "                        value_vars = [c for c in roll_batch.columns if c not in ('item_id', 'store_id')]\n",
    "                        \n",
    "                        if len(value_vars) == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        roll_melt = pd.melt(roll_batch, \n",
    "                                          id_vars=['item_id', 'store_id'],\n",
    "                                          value_vars=value_vars,\n",
    "                                          var_name='date',\n",
    "                                          value_name=name)\n",
    "                        \n",
    "                        roll_melt['date'] = roll_melt['date'].astype(str)\n",
    "                        all_features.append(roll_melt)\n",
    "                        \n",
    "                        del roll_batch, roll_melt\n",
    "                        gc.collect()\n",
    "                        feature_created = True\n",
    "                        \n",
    "                    if feature_created:\n",
    "                        print(f\"Feature created named := {name} (cols {keep_start}:{keep_end})\")\n",
    "                    \n",
    "                except MemoryError:\n",
    "                    print(f\"Memory error encountered for {name}, chunk {keep_start}:{keep_end}. Skipping...\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    # Clean up\n",
    "                    del sub_df, roll\n",
    "                    if 'roll_sel' in locals():\n",
    "                        del roll_sel\n",
    "                    gc.collect()\n",
    "            \n",
    "            if feature_created and all_features:\n",
    "                try:\n",
    "                    # Combine all features for this rolling window\n",
    "                    print(f\"Combining features for {name}...\")\n",
    "                    combined_features = pd.concat(all_features, ignore_index=True)\n",
    "                    combined_features = combined_features.drop_duplicates(['item_id', 'store_id', 'date'])\n",
    "                    \n",
    "                    # Process one dataset at a time to manage memory\n",
    "                    print(f\"Merging {name} into train...\")\n",
    "                    train = safe_merge(train, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into cv...\")\n",
    "                    cv = safe_merge(cv, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into test...\")\n",
    "                    test = safe_merge(test, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into final_test...\")\n",
    "                    final_test = safe_merge(final_test, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    created_features.add(name)\n",
    "                    print(f\"Successfully added feature {name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {name}: {str(e)}\")\n",
    "                finally:\n",
    "                    del combined_features\n",
    "                    gc.collect()\n",
    "            \n",
    "            del all_features\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3c5af",
   "metadata": {},
   "source": [
    "Exponential Weighted Average (EWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 0:500)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 500:1000)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 500:1000)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 1000:1500)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 1000:1500)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 1500:1969)\n",
      "Direct Feature created ewa window of size ewa_alpha_95_shift_28 (cols 1500:1969)\n",
      "Combining features for ewa_alpha_95_shift_28...\n",
      "Combining features for ewa_alpha_95_shift_28...\n",
      "Merging ewa_alpha_95_shift_28 into train...\n",
      "Merging ewa_alpha_95_shift_28 into train...\n",
      "Merging ewa_alpha_95_shift_28 into cv...\n",
      "Merging ewa_alpha_95_shift_28 into cv...\n",
      "Merging ewa_alpha_95_shift_28 into test...\n",
      "Merging ewa_alpha_95_shift_28 into test...\n",
      "Merging ewa_alpha_95_shift_28 into final_test...\n",
      "Merging ewa_alpha_95_shift_28 into final_test...\n",
      "Successfully added feature ewa_alpha_95_shift_28\n",
      "Successfully added feature ewa_alpha_95_shift_28\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 0:500)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 0:500)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 500:1000)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 500:1000)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 1000:1500)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 1000:1500)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 1500:1969)\n",
      "Direct Feature created ewa window of size ewa_alpha_90_shift_28 (cols 1500:1969)\n",
      "Combining features for ewa_alpha_90_shift_28...\n",
      "Combining features for ewa_alpha_90_shift_28...\n",
      "CPU times: total: 1min 5s\n",
      "Wall time: 1min 7s\n",
      "CPU times: total: 1min 5s\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_19756\\756052533.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_ipython().run_cell_magic(\u001b[33m'time'\u001b[39m, \u001b[33m''\u001b[39m, \u001b[33m'# Reduce df memory before heavy operations\\nif df.values.dtype != \\'float16\\':  # Use float16 instead of float32 to reduce memory\\n    df = df.astype(\\'float16\\')\\n\\ndate_cols = list(df.columns)\\nncols = len(date_cols)\\nchunk_size = 500  # Reduced chunk size to prevent memory issues\\n\\n# Track created features to avoid duplicates\\ncreated_features = set()\\n\\n# Function to safely merge features\\ndef safe_merge(df, features, name):\\n    # Drop the feature column if it already exists to avoid conflicts\\n    if name in df.columns:\\n        df = df.drop(columns=[name])\\n    return df.merge(features, on=[\\'item_id\\', \\'store_id\\', \\'date\\'], how=\\'left\\')\\n\\n# Different alpha values for EWA\\nalphas = [0.95, 0.9, 0.8, 0.7, 0.6]\\nshifts = [28]  # Keeping consistent with previous features\\n\\nfor alpha in alphas:\\n    for shif in shifts:\\n        name = f\"ewa_alpha_{int(alpha*100)}_shift_{shif}\"\\n        if name in created_features:\\n            print(f\"Skipping {name} - already created\")\\n            continue\\n\\n        feature_created = False\\n        all_features = []\\n\\n        for start in range(0, ncols, chunk_size):\\n            # Clear memory at start of each iteration\\n            gc.collect()\\n\\n            right = min(ncols, start + chunk_size)\\n            keep_start = start\\n            keep_end = min(start + chunk_size, ncols)\\n\\n            try:\\n                # Get subset of columns\\n                sub_cols = date_cols[:right]  # Include all previous data for proper EWA calculation\\n                sub_df = df.loc[:, sub_cols].copy()\\n\\n                # Compute EWA\\n                ewa = sub_df.ewm(alpha=alpha, adjust=False).mean().shift(shif, axis=1)\\n\\n                # Keep only the needed columns\\n                keep_cols = date_cols[keep_start:keep_end]\\n                ewa_sel = ewa.loc[:, [c for c in keep_cols if c in ewa.columns]]\\n\\n                if ewa_sel.shape[1] == 0:\\n                    del sub_df, ewa, ewa_sel\\n                    continue\\n\\n                # Process in smaller batches for melting\\n                batch_size = 50000\\n                n_batches = (len(ewa_sel) + batch_size - 1) // batch_size\\n\\n                for b in range(n_batches):\\n                    start_idx = b * batch_size\\n                    end_idx = min((b + 1) * batch_size, len(ewa_sel))\\n\\n                    # Process batch\\n                    ewa_batch = ewa_sel.iloc[start_idx:end_idx].reset_index()\\n                    value_vars = [c for c in ewa_batch.columns if c not in (\\'item_id\\', \\'store_id\\')]\\n\\n                    if len(value_vars) == 0:\\n                        continue\\n\\n                    ewa_melt = pd.melt(ewa_batch, \\n                                      id_vars=[\\'item_id\\', \\'store_id\\'],\\n                                      value_vars=value_vars,\\n                                      var_name=\\'date\\',\\n                                      value_name=name)\\n\\n                    ewa_melt[\\'date\\'] = ewa_melt[\\'date\\'].astype(str)\\n                    all_features.append(ewa_melt)\\n\\n                    del ewa_batch, ewa_melt\\n                    gc.collect()\\n                    feature_created = True\\n\\n                if feature_created:\\n                    print(f\"Direct Feature created ewa window of size {name} (cols {keep_start}:{keep_end})\")\\n\\n            except MemoryError:\\n                print(f\"Memory error encountered for {name}, chunk {keep_start}:{keep_end}. Skipping...\")\\n                continue\\n            finally:\\n                # Clean up\\n                del sub_df, ewa\\n                if \\'ewa_sel\\' in locals():\\n                    del ewa_sel\\n                gc.collect()\\n\\n        if feature_created and all_features:\\n            try:\\n                # Combine all features for this EWA window\\n                print(f\"Combining features for {name}...\")\\n                combined_features = pd.concat(all_features, ignore_index=True)\\n                combined_features = combined_features.drop_duplicates([\\'item_id\\', \\'store_id\\', \\'date\\'])\\n\\n                # Process one dataset at a time to manage memory\\n                print(f\"Merging {name} into train...\")\\n                train = safe_merge(train, combined_features, name)\\n                gc.collect()\\n\\n                print(f\"Merging {name} into cv...\")\\n                cv = safe_merge(cv, combined_features, name)\\n                gc.collect()\\n\\n                print(f\"Merging {name} into test...\")\\n                test = safe_merge(test, combined_features, name)\\n                gc.collect()\\n\\n                print(f\"Merging {name} into final_test...\")\\n                final_test = safe_merge(final_test, combined_features, name)\\n                gc.collect()\\n\\n                created_features.add(name)\\n                print(f\"Successfully added feature {name}\")\\n\\n            except Exception as e:\\n                print(f\"Error processing {name}: {str(e)}\")\\n            finally:\\n                del combined_features\\n                gc.collect()\\n\\n        del all_features\\n        gc.collect()\\n'\u001b[39m)\n",
      "\u001b[32m<timed exec>\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32mc:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep, inplace, ignore_index)\u001b[39m\n\u001b[32m   6836\u001b[39m \n\u001b[32m   6837\u001b[39m         inplace = validate_bool_kwarg(inplace, \u001b[33m\"inplace\"\u001b[39m)\n\u001b[32m   6838\u001b[39m         ignore_index = validate_bool_kwarg(ignore_index, \u001b[33m\"ignore_index\"\u001b[39m)\n\u001b[32m   6839\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m6840\u001b[39m         result = self[-self.duplicated(subset, keep=keep)]\n\u001b[32m   6841\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   6842\u001b[39m             result.index = default_index(len(result))\n\u001b[32m   6843\u001b[39m \n",
      "\u001b[32mc:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, subset, keep)\u001b[39m\n\u001b[32m   6979\u001b[39m             vals = (col.values \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;28;01min\u001b[39;00m self.items() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m subset)\n\u001b[32m   6980\u001b[39m             labels, shape = map(list, zip(*map(f, vals)))\n\u001b[32m   6981\u001b[39m \n\u001b[32m   6982\u001b[39m             ids = get_group_index(labels, tuple(shape), sort=\u001b[38;5;28;01mFalse\u001b[39;00m, xnull=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6983\u001b[39m             result = self._constructor_sliced(duplicated(ids, keep), index=self.index)\n\u001b[32m   6984\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result.__finalize__(self, method=\u001b[33m\"duplicated\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Memory optimization - convert to float16\n",
    "if df.values.dtype != 'float16':\n",
    "    df = df.astype('float16')\n",
    "\n",
    "# Different alpha values for EWA\n",
    "alphas = [0.99, 0.95, 0.9, 0.8, 0.7]\n",
    "shift = 28\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # Calculate EWA with shift\n",
    "        roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n",
    "        dates = roll.columns\n",
    "        \n",
    "        # Convert to float16 to save memory\n",
    "        roll = roll.astype('float16')\n",
    "        \n",
    "        # Reset index and melt\n",
    "        roll.reset_index(level=[0,1], inplace=True)\n",
    "        roll = pd.melt(roll,\n",
    "                      id_vars=['item_id', 'store_id'],\n",
    "                      value_vars=dates,\n",
    "                      var_name='date',\n",
    "                      value_name=f'ewa_alpha_{int(alpha*100)}_shift_{shift}')\n",
    "        \n",
    "        # Fill NaN values\n",
    "        roll.fillna(-1, inplace=True)\n",
    "        \n",
    "        # Merge with all datasets\n",
    "        print(f\"Merging alpha={alpha} features...\")\n",
    "        train = train.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        cv = cv.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        test = test.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        final_test = final_test.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        \n",
    "        print(f\"Direct Feature created ewa window of size alpha={alpha}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing alpha={alpha}: {str(e)}\")\n",
    "    finally:\n",
    "        del roll\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
