{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7fae47",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b976ee0",
   "metadata": {},
   "source": [
    "## Definition and a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814d5eb",
   "metadata": {},
   "source": [
    "Feature engineering - the process of transforming raw data into meaningful input features that better represent the underlying problem, improving the performance and accuracy of machine learning models. This critical data science technique involves selecting, creating, and transforming variables to enhance the data's predictive power and make it more suitable for algorithms to learn from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8519009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2043f4f",
   "metadata": {},
   "source": [
    "First of all, let's read up the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affe9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\2187268715.py:1: DtypeWarning: Columns (14,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train=pd.read_csv('final_dataframe.csv')\n",
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\2187268715.py:2: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test=pd.read_csv('final_dataframe_test.csv')\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('final_dataframe.csv')\n",
    "test=pd.read_csv('final_dataframe_test.csv')\n",
    "final_test=pd.read_csv('final_future_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa563a3",
   "metadata": {},
   "source": [
    "It took more than 10 minutes to read all the dataframes. It would be easier if I reduce the memory of all of those by converting all categorical variables to integer. Also, we save here label encoders data so we can use them to encode our future unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4590d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['item_id']=lbl.fit_transform(train['item_id'])\n",
    "test['item_id']=lbl.transform(test['item_id'])\n",
    "final_test['item_id']=lbl.transform(final_test['item_id'])\n",
    "pickle.dump(lbl,open('label_encoder_item_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcd916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['dept_id']=lbl.fit_transform(train['dept_id'])\n",
    "test['dept_id']=lbl.transform(test['dept_id'])\n",
    "final_test['dept_id']=lbl.transform(final_test['dept_id'])\n",
    "pickle.dump(lbl,open('label_encoder_dept_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e51df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['cat_id']=lbl.fit_transform(train['cat_id'])\n",
    "test['cat_id']=lbl.transform(test['cat_id'])\n",
    "final_test['cat_id']=lbl.transform(final_test['cat_id'])\n",
    "pickle.dump(lbl,open('label_encoder_cat_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4cbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['store_id']=lbl.fit_transform(train['store_id'])\n",
    "test['store_id']=lbl.transform(test['store_id'])\n",
    "final_test['store_id']=lbl.transform(final_test['store_id'])\n",
    "pickle.dump(lbl,open('label_encoder_store_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7beb2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['state_id']=lbl.fit_transform(train['state_id'])\n",
    "test['state_id']=lbl.transform(test['state_id'])\n",
    "final_test['state_id']=lbl.transform(final_test['state_id'])\n",
    "pickle.dump(lbl,open('label_encoder_state_id.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4febd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_name_1 encoding\n",
    "train['event_name_1'] = train['event_name_1'].fillna('no_event')\n",
    "test['event_name_1'] = test['event_name_1'].fillna('no_event')\n",
    "final_test['event_name_1'] = final_test['event_name_1'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_name_1'] = train['event_name_1'].astype(str)\n",
    "test['event_name_1'] = test['event_name_1'].astype(str)\n",
    "final_test['event_name_1'] = final_test['event_name_1'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_name_1'].values,\n",
    "    test['event_name_1'].values,\n",
    "    final_test['event_name_1'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_name_1'] = lbl.transform(train['event_name_1'])\n",
    "test['event_name_1'] = lbl.transform(test['event_name_1'])\n",
    "final_test['event_name_1'] = lbl.transform(final_test['event_name_1'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_name_1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a6c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_name_2 encoding\n",
    "train['event_name_2'] = train['event_name_2'].fillna('no_event')\n",
    "test['event_name_2'] = test['event_name_2'].fillna('no_event')\n",
    "final_test['event_name_2'] = final_test['event_name_2'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_name_2'] = train['event_name_2'].astype(str)\n",
    "test['event_name_2'] = test['event_name_2'].astype(str)\n",
    "final_test['event_name_2'] = final_test['event_name_2'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_name_2'].values,\n",
    "    test['event_name_2'].values,\n",
    "    final_test['event_name_2'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_name_2'] = lbl.transform(train['event_name_2'])\n",
    "test['event_name_2'] = lbl.transform(test['event_name_2'])\n",
    "final_test['event_name_2'] = lbl.transform(final_test['event_name_2'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_name_2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94410721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_type_1 encoding\n",
    "train['event_type_1'] = train['event_type_1'].fillna('no_event')\n",
    "test['event_type_1'] = test['event_type_1'].fillna('no_event')\n",
    "final_test['event_type_1'] = final_test['event_type_1'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_type_1'] = train['event_type_1'].astype(str)\n",
    "test['event_type_1'] = test['event_type_1'].astype(str)\n",
    "final_test['event_type_1'] = final_test['event_type_1'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_type_1'].values,\n",
    "    test['event_type_1'].values,\n",
    "    final_test['event_type_1'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_type_1'] = lbl.transform(train['event_type_1'])\n",
    "test['event_type_1'] = lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1'] = lbl.transform(final_test['event_type_1'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_type_1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f08ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle event_type_2 encoding\n",
    "train['event_type_2'] = train['event_type_2'].fillna('no_event')\n",
    "test['event_type_2'] = test['event_type_2'].fillna('no_event')\n",
    "final_test['event_type_2'] = final_test['event_type_2'].fillna('no_event')\n",
    "\n",
    "# Ensure all values are strings\n",
    "train['event_type_2'] = train['event_type_2'].astype(str)\n",
    "test['event_type_2'] = test['event_type_2'].astype(str)\n",
    "final_test['event_type_2'] = final_test['event_type_2'].astype(str)\n",
    "\n",
    "# Combine all values for fitting\n",
    "all_values = np.concatenate([\n",
    "    train['event_type_2'].values,\n",
    "    test['event_type_2'].values,\n",
    "    final_test['event_type_2'].values\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(all_values)\n",
    "train['event_type_2'] = lbl.transform(train['event_type_2'])\n",
    "test['event_type_2'] = lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2'] = lbl.transform(final_test['event_type_2'])\n",
    "\n",
    "pickle.dump(lbl, open('label_encoder_event_type_2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a423f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_1']=train['event_type_1'].fillna('no_event')\n",
    "test['event_type_1']=test['event_type_1'].fillna('no_event')\n",
    "final_test['event_type_1']=final_test['event_type_1'].fillna('no_event')\n",
    "train['event_type_1']=lbl.fit_transform(train['event_type_1'])\n",
    "test['event_type_1']=lbl.transform(test['event_type_1'])\n",
    "final_test['event_type_1']=lbl.transform(final_test['event_type_1'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_1.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f51351",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['event_type_2']=train['event_type_2'].fillna('no_event')\n",
    "test['event_type_2']=test['event_type_2'].fillna('no_event')\n",
    "final_test['event_type_2']=final_test['event_type_2'].fillna('no_event')\n",
    "train['event_type_2']=lbl.fit_transform(train['event_type_2'])\n",
    "test['event_type_2']=lbl.transform(test['event_type_2'])\n",
    "final_test['event_type_2']=lbl.transform(final_test['event_type_2'])\n",
    "pickle.dump(lbl,open('label_encoder_event_type_2.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ba4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "train['year']=lbl.fit_transform(train['year'])\n",
    "test['year']=lbl.transform(test['year'])\n",
    "final_test['year']=lbl.transform(final_test['year'])\n",
    "pickle.dump(lbl,open('label_encoder_year.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954ffcc",
   "metadata": {},
   "source": [
    "After the data reducing has been done, we can remove unnecessary columns. Firstly, let's convert all 3 state SNAPs into one feature named SNAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de4f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 56s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.loc[train['state_id'] == 'CA', 'snap'] = train.loc[train['state_id'] == 'CA']['snap_CA']\n",
    "train.loc[train['state_id'] == 'TX', 'snap'] = train.loc[train['state_id'] == 'TX']['snap_TX']\n",
    "train.loc[train['state_id'] == 'WI', 'snap'] = train.loc[train['state_id'] == 'WI']['snap_WI']\n",
    "train.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "test.loc[test['state_id'] == 'CA', 'snap'] = test.loc[test['state_id'] == 'CA']['snap_CA']\n",
    "test.loc[test['state_id'] == 'TX', 'snap'] = test.loc[test['state_id'] == 'TX']['snap_TX']\n",
    "test.loc[test['state_id'] == 'WI', 'snap'] = test.loc[test['state_id'] == 'WI']['snap_WI']\n",
    "test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)\n",
    "\n",
    "final_test.loc[final_test['state_id'] == 'CA', 'snap'] = final_test.loc[final_test['state_id'] == 'CA']['snap_CA']\n",
    "final_test.loc[final_test['state_id'] == 'TX', 'snap'] = final_test.loc[final_test['state_id'] == 'TX']['snap_TX']\n",
    "final_test.loc[final_test['state_id'] == 'WI', 'snap'] = final_test.loc[final_test['state_id'] == 'WI']['snap_WI']\n",
    "final_test.drop(['snap_CA','snap_TX','snap_WI'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f904d5e",
   "metadata": {},
   "source": [
    "Weekday = wday are similar features so there is no need to keep it. The same reason for having wm_yr_wk feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe5713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 44s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.drop('weekday',axis=1,inplace=True)\n",
    "train.drop('wm_yr_wk',axis=1,inplace=True)\n",
    " \n",
    "test.drop('weekday',axis=1,inplace=True)\n",
    "test.drop('wm_yr_wk',axis=1,inplace=True)\n",
    "\n",
    "final_test.drop('weekday',axis=1,inplace=True)\n",
    "final_test.drop('wm_yr_wk',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fc87d",
   "metadata": {},
   "source": [
    "FEATURES THAT INCLUDE TIME INTERVALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d35b4a",
   "metadata": {},
   "source": [
    "a) Number of the week - I created the function to get the week number of particular date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "460cde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_number(x):\n",
    "    date=calendar.datetime.date.fromisoformat(x)\n",
    "    return date.isocalendar()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a85b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['week_number']=train['date'].apply(lambda x:get_week_number(x))\n",
    "test['week_number']=test['date'].apply(lambda x:get_week_number(x))\n",
    "final_test['week_number']=final_test['date'].apply(lambda x:get_week_number(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e3e1d",
   "metadata": {},
   "source": [
    "b) Season of the year - A function that is used to get season according to the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22a7532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(x):\n",
    "    if x in [12,1,2]:\n",
    "        return 0      #\"Winter\"\n",
    "    elif x in [3,4,5]:\n",
    "        return 1   #\"Spring\"\n",
    "    elif x in [6,7,8]:\n",
    "        return 2   #\"Summer\"\n",
    "    else:\n",
    "        return 3   #\"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d28817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['season']=train['month'].apply(lambda x:get_season(x))\n",
    "test['season']=test['month'].apply(lambda x:get_season(x))\n",
    "final_test['season']=final_test['month'].apply(lambda x:get_season(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f89bf5",
   "metadata": {},
   "source": [
    "c) Start of a quarter - A function used to check which day starts the quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76a4a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quarter_begin(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and (month in [1,4,7,9])) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e6264cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quarter_start']=train['date'].apply(lambda x:check_if_quarter_begin(x))\n",
    "test['quarter_start']=test['date'].apply(lambda x:check_if_quarter_begin(x))\n",
    "final_test['quarter_start']=final_test['date'].apply(lambda x:check_if_quarter_begin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d75b4d",
   "metadata": {},
   "source": [
    "d) End of a quarter - A function used to check which day ends the quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61e2d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_quarter_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    if (day==31 and month==3) or (day==30 and month==6) or (day==30 and month==9) or (day==31 and month==12):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ed262f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['quarter_end']=train['date'].apply(lambda x:check_if_quarter_end(x))\n",
    "test['quarter_end']=test['date'].apply(lambda x:check_if_quarter_end(x))\n",
    "final_test['quarter_end']=final_test['date'].apply(lambda x:check_if_quarter_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e38e6",
   "metadata": {},
   "source": [
    "e) Start of a month - The function below checks if the day is beginning of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b4d6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def month_start(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    return 1 if day==1 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05881d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month_start']=train['date'].apply(lambda x:month_start(x))\n",
    "test['month_start']=test['date'].apply(lambda x:month_start(x))\n",
    "final_test['month_start']=final_test['date'].apply(lambda x:month_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d5ed9",
   "metadata": {},
   "source": [
    "f) End of a month - The function below checks if the day is end of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c31f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    year=calendar.datetime.date.fromisoformat(x).year\n",
    "    leap_yr=(year%4==0) # Checking if it is a leap year\n",
    "    val=(day==31 and month==1) or (day==29 if leap_yr else day==28) or (day==31 and month==3) or (day==30 and month==4) or\\\n",
    "        (day==31 and month==5) or (day==30 and month==6) or (day==31 and month==7) or (day==31 and month==8) or\\\n",
    "        (day==30 and month==9) or (day==31 and month==10) or (day==30 and month==11) or (day==31 and month==12)\n",
    "    return 1 if val else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d54e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['month_end']=train['date'].apply(lambda x:month_end(x))\n",
    "test['month_end']=test['date'].apply(lambda x:month_end(x))\n",
    "final_test['month_end']=final_test['date'].apply(lambda x:month_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaf4b8",
   "metadata": {},
   "source": [
    "g) Start of a year - The function checking if a given day is the beginning of a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c808667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_start(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==1 and month==1) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f3bb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_start']=train['date'].apply(lambda x:year_start(x))\n",
    "test['year_start']=test['date'].apply(lambda x:year_start(x))\n",
    "final_test['year_start']=final_test['date'].apply(lambda x:year_start(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bf558",
   "metadata": {},
   "source": [
    "h) End of a year - The function checking if a given day is the end of a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6197d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_end(x):\n",
    "    day=calendar.datetime.date.fromisoformat(x).day\n",
    "    month=calendar.datetime.date.fromisoformat(x).month\n",
    "    return 1 if (day==31 and month==12) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb2e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['year_end']=train['date'].apply(lambda x:year_end(x))\n",
    "test['year_end']=test['date'].apply(lambda x:year_end(x))\n",
    "final_test['year_end']=final_test['date'].apply(lambda x:year_end(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cf509",
   "metadata": {},
   "source": [
    "We can take the last 28 days from the train data for cross validation that could be used for further modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5bb4c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=train[train['date']>='2016-03-28']\n",
    "train=train[train['date']<'2016-03-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc779589",
   "metadata": {},
   "source": [
    "Last but not least, I will create a time series related features. I am going to create direct feature to test and train data. Below I wrote a code that creates a large data for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f398c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 55s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "tt=pd.concat([train,cv,test,final_test])\n",
    "tt.sort_values(['id','date'],inplace=True)\n",
    "df=tt.pivot_table(index=['item_id','store_id'],columns='date',values='sales')\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a74c37",
   "metadata": {},
   "source": [
    "My next step that I will take will be the calculation of a rolling mean and standard deviation. I also took 28 days based on requirements and to avoid the data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d500500a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_7_shift_28_mean...\n",
      "Merging roll_7_shift_28_mean into train...\n",
      "Merging roll_7_shift_28_mean into train...\n",
      "Merging roll_7_shift_28_mean into cv...\n",
      "Merging roll_7_shift_28_mean into cv...\n",
      "Merging roll_7_shift_28_mean into test...\n",
      "Merging roll_7_shift_28_mean into test...\n",
      "Merging roll_7_shift_28_mean into final_test...\n",
      "Merging roll_7_shift_28_mean into final_test...\n",
      "Successfully added feature roll_7_shift_28_mean\n",
      "Successfully added feature roll_7_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_14_shift_28_mean...\n",
      "Combining features for roll_14_shift_28_mean...\n",
      "Merging roll_14_shift_28_mean into train...\n",
      "Merging roll_14_shift_28_mean into train...\n",
      "Merging roll_14_shift_28_mean into cv...\n",
      "Merging roll_14_shift_28_mean into cv...\n",
      "Merging roll_14_shift_28_mean into test...\n",
      "Merging roll_14_shift_28_mean into test...\n",
      "Merging roll_14_shift_28_mean into final_test...\n",
      "Merging roll_14_shift_28_mean into final_test...\n",
      "Successfully added feature roll_14_shift_28_mean\n",
      "Successfully added feature roll_14_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_30_shift_28_mean...\n",
      "Combining features for roll_30_shift_28_mean...\n",
      "Merging roll_30_shift_28_mean into train...\n",
      "Merging roll_30_shift_28_mean into train...\n",
      "Merging roll_30_shift_28_mean into cv...\n",
      "Merging roll_30_shift_28_mean into cv...\n",
      "Merging roll_30_shift_28_mean into test...\n",
      "Merging roll_30_shift_28_mean into test...\n",
      "Merging roll_30_shift_28_mean into final_test...\n",
      "Merging roll_30_shift_28_mean into final_test...\n",
      "Successfully added feature roll_30_shift_28_mean\n",
      "Successfully added feature roll_30_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_60_shift_28_mean...\n",
      "Combining features for roll_60_shift_28_mean...\n",
      "Merging roll_60_shift_28_mean into train...\n",
      "Merging roll_60_shift_28_mean into train...\n",
      "Merging roll_60_shift_28_mean into cv...\n",
      "Merging roll_60_shift_28_mean into cv...\n",
      "Merging roll_60_shift_28_mean into test...\n",
      "Merging roll_60_shift_28_mean into test...\n",
      "Merging roll_60_shift_28_mean into final_test...\n",
      "Merging roll_60_shift_28_mean into final_test...\n",
      "Successfully added feature roll_60_shift_28_mean\n",
      "Successfully added feature roll_60_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_mean (cols 1500:1969)\n",
      "Combining features for roll_360_shift_28_mean...\n",
      "Combining features for roll_360_shift_28_mean...\n",
      "Merging roll_360_shift_28_mean into train...\n",
      "Merging roll_360_shift_28_mean into train...\n",
      "Merging roll_360_shift_28_mean into cv...\n",
      "Merging roll_360_shift_28_mean into cv...\n",
      "Merging roll_360_shift_28_mean into test...\n",
      "Merging roll_360_shift_28_mean into test...\n",
      "Merging roll_360_shift_28_mean into final_test...\n",
      "Merging roll_360_shift_28_mean into final_test...\n",
      "Successfully added feature roll_360_shift_28_mean\n",
      "Successfully added feature roll_360_shift_28_mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_7_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_7_shift_28_std...\n",
      "Combining features for roll_7_shift_28_std...\n",
      "Merging roll_7_shift_28_std into train...\n",
      "Merging roll_7_shift_28_std into train...\n",
      "Merging roll_7_shift_28_std into cv...\n",
      "Merging roll_7_shift_28_std into cv...\n",
      "Merging roll_7_shift_28_std into test...\n",
      "Merging roll_7_shift_28_std into test...\n",
      "Merging roll_7_shift_28_std into final_test...\n",
      "Merging roll_7_shift_28_std into final_test...\n",
      "Successfully added feature roll_7_shift_28_std\n",
      "Successfully added feature roll_7_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_14_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_14_shift_28_std...\n",
      "Combining features for roll_14_shift_28_std...\n",
      "Merging roll_14_shift_28_std into train...\n",
      "Merging roll_14_shift_28_std into train...\n",
      "Merging roll_14_shift_28_std into cv...\n",
      "Merging roll_14_shift_28_std into cv...\n",
      "Merging roll_14_shift_28_std into test...\n",
      "Merging roll_14_shift_28_std into test...\n",
      "Merging roll_14_shift_28_std into final_test...\n",
      "Merging roll_14_shift_28_std into final_test...\n",
      "Successfully added feature roll_14_shift_28_std\n",
      "Successfully added feature roll_14_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_30_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_30_shift_28_std...\n",
      "Merging roll_30_shift_28_std into train...\n",
      "Merging roll_30_shift_28_std into train...\n",
      "Merging roll_30_shift_28_std into cv...\n",
      "Merging roll_30_shift_28_std into cv...\n",
      "Merging roll_30_shift_28_std into test...\n",
      "Merging roll_30_shift_28_std into test...\n",
      "Merging roll_30_shift_28_std into final_test...\n",
      "Merging roll_30_shift_28_std into final_test...\n",
      "Successfully added feature roll_30_shift_28_std\n",
      "Successfully added feature roll_30_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_60_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_60_shift_28_std...\n",
      "Merging roll_60_shift_28_std into train...\n",
      "Merging roll_60_shift_28_std into train...\n",
      "Merging roll_60_shift_28_std into cv...\n",
      "Merging roll_60_shift_28_std into cv...\n",
      "Merging roll_60_shift_28_std into test...\n",
      "Merging roll_60_shift_28_std into test...\n",
      "Merging roll_60_shift_28_std into final_test...\n",
      "Merging roll_60_shift_28_std into final_test...\n",
      "Successfully added feature roll_60_shift_28_std\n",
      "Successfully added feature roll_60_shift_28_std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 0:500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 500:1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 1000:1500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:46: FutureWarning: Support for axis=1 in DataFrame.rolling is deprecated and will be removed in a future version. Use obj.T.rolling(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created named := roll_360_shift_28_std (cols 1500:1969)\n",
      "Combining features for roll_360_shift_28_std...\n",
      "Combining features for roll_360_shift_28_std...\n",
      "Merging roll_360_shift_28_std into train...\n",
      "Merging roll_360_shift_28_std into train...\n",
      "Merging roll_360_shift_28_std into cv...\n",
      "Merging roll_360_shift_28_std into cv...\n",
      "Merging roll_360_shift_28_std into test...\n",
      "Merging roll_360_shift_28_std into test...\n",
      "Merging roll_360_shift_28_std into final_test...\n",
      "Merging roll_360_shift_28_std into final_test...\n",
      "Successfully added feature roll_360_shift_28_std\n",
      "CPU times: total: 6min 57s\n",
      "Wall time: 7min\n",
      "Successfully added feature roll_360_shift_28_std\n",
      "CPU times: total: 6min 57s\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reduce df memory before heavy operations\n",
    "if df.values.dtype != 'float16':  # Use float16 instead of float32 to reduce memory\n",
    "    df = df.astype('float16')\n",
    "\n",
    "date_cols = list(df.columns)\n",
    "ncols = len(date_cols)\n",
    "chunk_size = 500  # Reduced chunk size to prevent memory issues\n",
    "\n",
    "# Track created features to avoid duplicates\n",
    "created_features = set()\n",
    "\n",
    "# Function to safely merge features\n",
    "def safe_merge(df, features, name):\n",
    "    # Drop the feature column if it already exists to avoid conflicts\n",
    "    if name in df.columns:\n",
    "        df = df.drop(columns=[name])\n",
    "    return df.merge(features, on=['item_id', 'store_id', 'date'], how='left')\n",
    "\n",
    "for aggregate in ['mean', 'std']:\n",
    "    for shif in [28]:\n",
    "        for r in [7, 14, 30, 60, 360]:\n",
    "            name = f\"roll_{r}_shift_{shif}_{aggregate}\"\n",
    "            if name in created_features:\n",
    "                print(f\"Skipping {name} - already created\")\n",
    "                continue\n",
    "                \n",
    "            pad = r - 1\n",
    "            feature_created = False\n",
    "            all_features = []\n",
    "            \n",
    "            for start in range(0, ncols, chunk_size):\n",
    "                # Clear memory at start of each iteration\n",
    "                gc.collect()\n",
    "                \n",
    "                left = max(0, start - pad)\n",
    "                right = min(ncols, start + chunk_size)\n",
    "                keep_start = start\n",
    "                keep_end = min(start + chunk_size, ncols)\n",
    "\n",
    "                try:\n",
    "                    # Get subset of columns including padding\n",
    "                    sub_cols = date_cols[left:right]\n",
    "                    sub_df = df.loc[:, sub_cols].copy()  # Make an explicit copy\n",
    "\n",
    "                    # Compute rolling stats\n",
    "                    roll = sub_df.rolling(r, axis=1).agg(aggregate).shift(shif, axis=1)\n",
    "                    \n",
    "                    # Keep only the needed columns\n",
    "                    keep_cols = date_cols[keep_start:keep_end]\n",
    "                    roll_sel = roll.loc[:, [c for c in keep_cols if c in roll.columns]]\n",
    "                    \n",
    "                    if roll_sel.shape[1] == 0:\n",
    "                        del sub_df, roll, roll_sel\n",
    "                        continue\n",
    "\n",
    "                    # Process in smaller batches for melting\n",
    "                    batch_size = 50000  # Reduced batch size\n",
    "                    n_batches = (len(roll_sel) + batch_size - 1) // batch_size\n",
    "                    \n",
    "                    for b in range(n_batches):\n",
    "                        start_idx = b * batch_size\n",
    "                        end_idx = min((b + 1) * batch_size, len(roll_sel))\n",
    "                        \n",
    "                        # Process batch\n",
    "                        roll_batch = roll_sel.iloc[start_idx:end_idx].reset_index()\n",
    "                        value_vars = [c for c in roll_batch.columns if c not in ('item_id', 'store_id')]\n",
    "                        \n",
    "                        if len(value_vars) == 0:\n",
    "                            continue\n",
    "                            \n",
    "                        roll_melt = pd.melt(roll_batch, \n",
    "                                          id_vars=['item_id', 'store_id'],\n",
    "                                          value_vars=value_vars,\n",
    "                                          var_name='date',\n",
    "                                          value_name=name)\n",
    "                        \n",
    "                        roll_melt['date'] = roll_melt['date'].astype(str)\n",
    "                        all_features.append(roll_melt)\n",
    "                        \n",
    "                        del roll_batch, roll_melt\n",
    "                        gc.collect()\n",
    "                        feature_created = True\n",
    "                        \n",
    "                    if feature_created:\n",
    "                        print(f\"Feature created named := {name} (cols {keep_start}:{keep_end})\")\n",
    "                    \n",
    "                except MemoryError:\n",
    "                    print(f\"Memory error encountered for {name}, chunk {keep_start}:{keep_end}. Skipping...\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    # Clean up\n",
    "                    del sub_df, roll\n",
    "                    if 'roll_sel' in locals():\n",
    "                        del roll_sel\n",
    "                    gc.collect()\n",
    "            \n",
    "            if feature_created and all_features:\n",
    "                try:\n",
    "                    # Combine all features for this rolling window\n",
    "                    print(f\"Combining features for {name}...\")\n",
    "                    combined_features = pd.concat(all_features, ignore_index=True)\n",
    "                    combined_features = combined_features.drop_duplicates(['item_id', 'store_id', 'date'])\n",
    "                    \n",
    "                    # Process one dataset at a time to manage memory\n",
    "                    print(f\"Merging {name} into train...\")\n",
    "                    train = safe_merge(train, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into cv...\")\n",
    "                    cv = safe_merge(cv, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into test...\")\n",
    "                    test = safe_merge(test, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Merging {name} into final_test...\")\n",
    "                    final_test = safe_merge(final_test, combined_features, name)\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    created_features.add(name)\n",
    "                    print(f\"Successfully added feature {name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {name}: {str(e)}\")\n",
    "                finally:\n",
    "                    del combined_features\n",
    "                    gc.collect()\n",
    "            \n",
    "            del all_features\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3c5af",
   "metadata": {},
   "source": [
    "Exponential Weighted Average (EWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7249bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\4171141136.py:15: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alpha=0.99 features...\n",
      "Direct Feature created ewa window of size alpha=0.99\n",
      "Direct Feature created ewa window of size alpha=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\4171141136.py:15: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alpha=0.95 features...\n",
      "Direct Feature created ewa window of size alpha=0.95\n",
      "Direct Feature created ewa window of size alpha=0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\4171141136.py:15: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alpha=0.9 features...\n",
      "Direct Feature created ewa window of size alpha=0.9\n",
      "Direct Feature created ewa window of size alpha=0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\4171141136.py:15: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alpha=0.8 features...\n",
      "Direct Feature created ewa window of size alpha=0.8\n",
      "Direct Feature created ewa window of size alpha=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wiktor\\AppData\\Local\\Temp\\ipykernel_19756\\4171141136.py:15: FutureWarning: Support for axis=1 in DataFrame.ewm is deprecated and will be removed in a future version. Use obj.T.ewm(...) instead\n",
      "  roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging alpha=0.7 features...\n",
      "Direct Feature created ewa window of size alpha=0.7\n",
      "Direct Feature created ewa window of size alpha=0.7\n"
     ]
    }
   ],
   "source": [
    "# Memory optimization - convert to float16\n",
    "if df.values.dtype != 'float16':\n",
    "    df = df.astype('float16')\n",
    "\n",
    "# Different alpha values for EWA\n",
    "alphas = [0.99, 0.95, 0.9, 0.8, 0.7]\n",
    "shift = 28\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # Calculate EWA with shift\n",
    "        roll = df.shift(shift, axis=1).ewm(alpha=alpha, axis=1, adjust=False).mean()\n",
    "        dates = roll.columns\n",
    "        \n",
    "        # Convert to float16 to save memory\n",
    "        roll = roll.astype('float16')\n",
    "        \n",
    "        # Reset index and melt\n",
    "        roll.reset_index(level=[0,1], inplace=True)\n",
    "        roll = pd.melt(roll,\n",
    "                      id_vars=['item_id', 'store_id'],\n",
    "                      value_vars=dates,\n",
    "                      var_name='date',\n",
    "                      value_name=f'ewa_alpha_{int(alpha*100)}_shift_{shift}')\n",
    "        \n",
    "        # Fill NaN values\n",
    "        roll.fillna(-1, inplace=True)\n",
    "        \n",
    "        # Merge with all datasets\n",
    "        print(f\"Merging alpha={alpha} features...\")\n",
    "        train = train.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        cv = cv.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        test = test.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        final_test = final_test.merge(roll, on=['item_id', 'store_id', 'date'])\n",
    "        \n",
    "        print(f\"Direct Feature created ewa window of size alpha={alpha}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing alpha={alpha}: {str(e)}\")\n",
    "    finally:\n",
    "        del roll\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3469ae7",
   "metadata": {},
   "source": [
    "Our last, but not least step will be calculating lag features with lag of 28,35,42,49,56,63,70,77,84,91,96 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f809efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created for lag 28\n",
      "Feature created for lag 35\n",
      "Feature created for lag 42\n",
      "Feature created for lag 49\n",
      "Feature created for lag 56\n",
      "Feature created for lag 63\n",
      "Feature created for lag 70\n",
      "Feature created for lag 77\n",
      "Feature created for lag 84\n",
      "Feature created for lag 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature created for lag 98\n",
      "CPU times: total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lag in range(28,100,7):\n",
    "    i='direct_lag_'+str(lag)\n",
    "    lag_i=df.shift(lag,axis=1)\n",
    "    dates=lag_i.columns\n",
    "    lag_i.reset_index(level=[0,1],inplace=True)\n",
    "    lag_i=pd.melt(lag_i,id_vars=['item_id','store_id'],value_vars=dates,var_name='date',value_name=i)\n",
    "    lag_i.fillna(-1,inplace=True)\n",
    "    lag_i[i]=lag_i[i].astype('int16')\n",
    "    train=train.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    cv=cv.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    test=test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    final_test=final_test.merge(lag_i,on=['item_id','store_id','date'])\n",
    "    print(\"Feature created for lag\",lag)\n",
    "    del lag_i\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb2753",
   "metadata": {},
   "source": [
    "Finally we can save all creatures created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2920bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.to_csv('train1.csv',index=False)\n",
    "cv.to_csv('cv1.csv',index=False)\n",
    "test.to_csv('test1.csv',index=False)\n",
    "final_test.to_csv('final_test1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d962e3f",
   "metadata": {},
   "source": [
    "All time series features have been constructed with a shift of 28 days in order to not get stuck into data leakage problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
