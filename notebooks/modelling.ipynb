{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cebb97",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3e7cd",
   "metadata": {},
   "source": [
    "As before, let's start with importing the most important libraries for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e013797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import dask.dataframe as dk\n",
    "import calendar\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostRegressor\n",
    "import catboost as cat\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6be273",
   "metadata": {},
   "source": [
    "I have it in my mind that I work on huge amount of data. So I am going to reduce memory consumption by dataframe using this snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dae761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={'id'       :     'object', \n",
    "    'item_id'     :  'int64', \n",
    "    'dept_id'     :  'int8', \n",
    "    'cat_id'      :  'int8', \n",
    "    'store_id'    :  'int8', \n",
    "    'state_id'    :  'int8', \n",
    "    'd'           :  'object', \n",
    "    'sales'       :  'int16',  \n",
    "    'date'        : 'object', \n",
    "   'wday'        :  'int8',  \n",
    "   'month'       :  'int8',  \n",
    "   'year'        :  'int16',  \n",
    "   'event_name_1' : 'int8', \n",
    "   'event_type_1' : 'int8', \n",
    "   'event_name_2' : 'int8', \n",
    "   'event_type_2' : 'int8', \n",
    "    'snap':'int8',\n",
    "  'sell_price'   : 'float16',\n",
    "       'price_change':'float16',\n",
    "   'week_number'  : 'int8',  \n",
    "   'season'       : 'object', \n",
    "   'quarter_start' : 'int8',  \n",
    "   'quarter_end'   : 'int8',  \n",
    "   'month_start'  : 'int8',  \n",
    "   'month_end'    : 'int8',  \n",
    "   'year_start'   : 'int8',  \n",
    "   'year_end'     : 'int8',  \n",
    "   'group'        : 'int8',  \n",
    "   'no_events'    : 'object', \n",
    "   'holiday'      : 'object',\n",
    "    'week_number':'int8',\n",
    "       'season':'int8',\n",
    "       'quarter_start':'int8',\n",
    "       'quarter_end':'int8',\n",
    "       'month_start':'int8',\n",
    "       'month_end':'int8',\n",
    "       'year_start':'int8',\n",
    "       'year_end':'int8',\n",
    "       'roll_7_shift_28_mean':'float16',\n",
    "       'roll_14_shift_28_mean':'float16',\n",
    "       'roll_30_shift_28_mean':'float16',\n",
    "       'roll_60_shift_28_mean':'float16',\n",
    "       'roll_360_shift_28_mean':'float16',\n",
    "       'roll_7_shift_28_std':'float16',\n",
    "       'roll_14_shift_28_std':'float16',\n",
    "       'roll_30_shift_28_std':'float16',\n",
    "       'roll_60_shift_28_std':'float16',\n",
    "       'roll_360_shift_28_std':'float16',\n",
    "       'direct_ewm':'float16',\n",
    "       'direct_lag_28':'int16',\n",
    "       'direct_lag_35':'int16',\n",
    "       'direct_lag_42':'int16',\n",
    "       'direct_lag_49':'int16',\n",
    "       'direct_lag_56':'int16',\n",
    "       'direct_lag_63':'int16',\n",
    "       'direct_lag_70':'int16',\n",
    "       'direct_lag_77':'int16',\n",
    "       'direct_lag_84':'int16',\n",
    "       'direct_lag_91':'int16',\n",
    "       'direct_lag_98':'int16',\n",
    "       'min_price':'float16',\n",
    "       'max_price':'float16',\n",
    "       'mean_price':'float16',\n",
    "       'std_price':'float16',\n",
    "       'price_norm_1':'float16',\n",
    "       'price_norm_2':'float16',\n",
    "       'price_norm_3':'float16',\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4885c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train1.csv',dtype=dtype)\n",
    "cv=pd.read_csv('cv1.csv',dtype=dtype)\n",
    "test=pd.read_csv('test1.csv',dtype=dtype)\n",
    "final_test=pd.read_csv('final_test1.csv',dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6d5f9",
   "metadata": {},
   "source": [
    "Before I start working on modelling data to estimate unit sales for 28 days, we can create metric calculation functions. The first function is going to be used to calculate the Root Mean Squared Scaled Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ac0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caluclate_WRMSSE(actual,predicted,train,weights,h,n):\n",
    "    num=((actual-predicted)**2).sum(axis=1)/h\n",
    "    denom=(train[:,1:]-train[:,:-1])**2\n",
    "    denom=denom.sum(axis=1)/(n-1)\n",
    "    return (num/denom)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24e464",
   "metadata": {},
   "source": [
    "Our next function is used to get Weighted Root Mean Squared Scaled Error that is used in this study as a metric for cross validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances(model,train,cv,test,X_cv,X_test):\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    cv['pred_sales']=model.predict(X_cv)\n",
    "    df1=cv.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1886,1914)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg=agg.append(temp_df[col])\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg=agg.append(dd[col])\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    cv.drop(['pred_sales'],axis=1,inplace=True)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    test['pred_sales']=model.predict(X_test)\n",
    "    df1=test.pivot_table(index=['id'],columns='d',values='pred_sales')\n",
    "    dic={}\n",
    "    for j,i in enumerate(range(1914,1942)):\n",
    "        dic['d_'+str(i)]='F'+str(j+1)\n",
    "    df1=df1.rename(columns=dic) \n",
    "    df1.reset_index(level=[0],inplace=True)\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg=agg.append(temp_df[col])\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg=agg.append(dd[col])\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f70433",
   "metadata": {},
   "source": [
    "The last function to create for metric calculation will be used for getting WRMSSE that is used in this case study as a metric for cross validation and test data where model is trained according to store's id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performances_for_store_wise_trained_model(train,cv,test,cv_pred,test_pred):\n",
    "    #For CV Data\n",
    "    cv['prices']=cv['sales']*cv['sell_price']\n",
    "    total_sales=cv.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i]=total_sales.loc[i]['sales']/total\n",
    "    train1=pd.concat([train,cv])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    \n",
    "    df1=cv_pred\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1914)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg=agg.append(temp_df[col])\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg=agg.append(dd[col])\n",
    "    actual=agg[['d_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891', 'd_1892',\n",
    "       'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898', 'd_1899',\n",
    "       'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1886)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1886)\n",
    "    cv_wrsmme=np.sum(rmsse*weights)\n",
    "    del actual,predicted,training,weights,agg\n",
    "    \n",
    "    #For Test data\n",
    "    test['prices']=test['sales']*test['sell_price']\n",
    "    total_sales=test.groupby('id').sum()\n",
    "    total=sum(total_sales['sales'])\n",
    "    weight={}\n",
    "    for i in total_sales.index:\n",
    "        weight[i.replace('evaluation','validation')]=total_sales.loc[i]['sales']/total\n",
    "    test['id']=test['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    train1=pd.concat([train,cv,test])\n",
    "    train1.sort_values(['id','date'],inplace=True)\n",
    "    train1.fillna(0,inplace=True)\n",
    "    df=train1.pivot_table(index=['id','state_id','store_id','cat_id','dept_id','item_id'],columns='d',values='sales')\n",
    "    df.reset_index(level=[0,1,2,3,4,5],inplace=True)\n",
    "    df.fillna(0,inplace=True)\n",
    "    del train1\n",
    "    import gc\n",
    "    df1=test_pred\n",
    "    df1['id']=df1['id'].apply(lambda x:x.replace('evaluation','validation'))\n",
    "    dd=df.merge(df1,on='id')\n",
    "    dd['weight']=dd['id'].apply(lambda x:weight[x])\n",
    "    l=['d_'+str(i) for i in range(1,1942)]\n",
    "    l1=['F'+str(i) for i in range(1,29)]\n",
    "    agg_level={2:['state_id'],3:['store_id'],4:['cat_id'],5:['dept_id'],6:['state_id','cat_id'],\\\n",
    "           7:['state_id','dept_id'],8:['store_id','cat_id'],9:['store_id','dept_id'],10:['item_id'],11:['item_id','state_id']}\n",
    "    agg=pd.DataFrame(dd[l+l1].sum()).transpose()\n",
    "    agg['weight']=1/12\n",
    "    agg['level']=1\n",
    "    col=agg.columns\n",
    "    for level in agg_level:\n",
    "        temp_df=dd.groupby(by=agg_level[level]).sum().reset_index(drop=True)\n",
    "        temp_df['weight']/=12\n",
    "        temp_df['level']=level\n",
    "        agg=agg.append(temp_df[col])\n",
    "    dd['weight']/=12\n",
    "    dd['level']=12    \n",
    "    agg=agg.append(dd[col])\n",
    "    actual=agg[['d_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919',\n",
    "       'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925',\n",
    "       'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931',\n",
    "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937',\n",
    "       'd_1938', 'd_1939', 'd_1940', 'd_1941']].values\n",
    "    predicted=agg[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11',\n",
    "       'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21',\n",
    "       'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']].values\n",
    "    training=agg[['d_'+str(i) for i in range(1,1914)]].values\n",
    "    weights=agg['weight'].values\n",
    "    rmsse=caluclate_WRMSSE(actual,predicted,training,weights,28,1914)\n",
    "    test_wrsmme=np.sum(rmsse*weights)\n",
    "\n",
    "    print(\"CV WRMSSE=\",cv_wrsmme)\n",
    "    print(\"Test WRMSSE=\",test_wrsmme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264953d7",
   "metadata": {},
   "source": [
    "NOW WE CAN START MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048809ff",
   "metadata": {},
   "source": [
    "a) Long Short-Term Memory Network (No Date Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05074222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input1=tf.keras.layers.Input(shape=(1,),name='Item_Id')\n",
    "input2=tf.keras.layers.Input(shape=(1,),name='Dept_Id')\n",
    "input3=tf.keras.layers.Input(shape=(1,),name='Cat_Id')\n",
    "input4=tf.keras.layers.Input(shape=(1,),name='Store_Id')\n",
    "input5=tf.keras.layers.Input(shape=(1,),name='State_Id')\n",
    "input6=tf.keras.layers.Input(shape=(1,),name='year')\n",
    "input7=tf.keras.layers.Input(shape=(1,),name='event_name_1')\n",
    "input8=tf.keras.layers.Input(shape=(1,),name='event_name_2')\n",
    "input9=tf.keras.layers.Input(shape=(1,),name='season')\n",
    "input10=tf.keras.layers.Input(shape=(1,23),name='Numerical_features')\n",
    "\n",
    "emb1=tf.keras.layers.Embedding(3050,output_dim=150)(input1)\n",
    "emb2=tf.keras.layers.Embedding(8,output_dim=10)(input2)\n",
    "emb3=tf.keras.layers.Embedding(4,output_dim=10)(input3)\n",
    "emb4=tf.keras.layers.Embedding(11,output_dim=10)(input4)\n",
    "emb5=tf.keras.layers.Embedding(4,output_dim=10)(input5)\n",
    "emb6=tf.keras.layers.Embedding(2017,output_dim=10)(input6)\n",
    "emb7=tf.keras.layers.Embedding(32,output_dim=10)(input7)\n",
    "emb8=tf.keras.layers.Embedding(6,output_dim=10)(input8)\n",
    "emb9=tf.keras.layers.Embedding(5,output_dim=10)(input9)\n",
    "\n",
    "lstm1=tf.keras.layers.LSTM(50)(emb1)\n",
    "lstm2=tf.keras.layers.LSTM(10)(emb2)\n",
    "lstm3=tf.keras.layers.LSTM(10)(emb3)\n",
    "lstm4=tf.keras.layers.LSTM(10)(emb4)\n",
    "lstm5=tf.keras.layers.LSTM(10)(emb5)\n",
    "lstm6=tf.keras.layers.LSTM(10)(emb6)\n",
    "lstm7=tf.keras.layers.LSTM(10)(emb7)\n",
    "lstm8=tf.keras.layers.LSTM(10)(emb8)\n",
    "lstm9=tf.keras.layers.LSTM(10)(emb9)\n",
    "lstm10=tf.keras.layers.LSTM(10)(input10)\n",
    "\n",
    "x1=tf.keras.layers.Flatten()(lstm1)\n",
    "x2=tf.keras.layers.Flatten()(lstm2)\n",
    "x3=tf.keras.layers.Flatten()(lstm3)\n",
    "x4=tf.keras.layers.Flatten()(lstm4)\n",
    "x5=tf.keras.layers.Flatten()(lstm5)\n",
    "x6=tf.keras.layers.Flatten()(lstm6)\n",
    "x7=tf.keras.layers.Flatten()(lstm7)\n",
    "x8=tf.keras.layers.Flatten()(lstm8)\n",
    "x9=tf.keras.layers.Flatten()(lstm9)\n",
    "x10=tf.keras.layers.Flatten()(input10)\n",
    "\n",
    "x=tf.keras.layers.Concatenate()([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10])\n",
    "x=tf.keras.layers.BatchNormalization()(x)\n",
    "x=tf.keras.layers.Dense(256,activation='sigmoid')(x)\n",
    "x=tf.keras.layers.Dense(128,activation='tanh')(x)\n",
    "x=tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(1)(x)\n",
    "model=tf.keras.Model([input1,input2,input3,input4,input5,input6,input7,input8,input9,input10],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba18f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.0009),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c099ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[train['item_id'].values.reshape(-1,1),train['dept_id'].values.reshape(-1,1),train['cat_id'].values.reshape(-1,1),train['store_id'].values.reshape(-1,1),\\\n",
    "             train['state_id'].values.reshape(-1,1),train['year'].values.reshape(-1,1),train['event_name_1'].values.reshape(-1,1),train['event_name_2'].values.reshape(-1,1),\\\n",
    "             train['season'].values.reshape(-1,1),train[[ 'roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],\\\n",
    "          y=train['sales'],epochs=10,verbose=2,batch_size=32,\\\n",
    "          validation_data=([cv['item_id'].values.reshape(-1,1),cv['dept_id'].values.reshape(-1,1),cv['cat_id'].values.reshape(-1,1),cv['store_id'].values.reshape(-1,1),\\\n",
    "             cv['state_id'].values.reshape(-1,1),cv['year'].values.reshape(-1,1),cv['event_name_1'].values.reshape(-1,1),cv['event_name_2'].values.reshape(-1,1),\\\n",
    "             cv['season'].values.reshape(-1,1),cv[[ 'roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],cv['sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc704ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_model_performances(model,train,cv,test,[cv['item_id'].values.reshape(-1,1),cv['dept_id'].values,cv['cat_id'].values.reshape(-1,1),cv['store_id'].values.reshape(-1,1),\\\n",
    "             cv['state_id'].values.reshape(-1,1),cv['year'].values.reshape(-1,1),cv['event_name_1'].values.reshape(-1,1),cv['event_name_2'].values.reshape(-1,1),\\\n",
    "             cv['season'].values.reshape(-1,1),cv[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],\\\n",
    "                       [test['item_id'].values.reshape(-1,1),test['dept_id'].values,test['cat_id'].values.reshape(-1,1),test['store_id'].values.reshape(-1,1),\\\n",
    "             test['state_id'].values.reshape(-1,1),test['year'].values.reshape(-1,1),test['event_name_1'].values.reshape(-1,1),test['event_name_2'].values.reshape(-1,1),\\\n",
    "             test['season'].values.reshape(-1,1),test[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339901df",
   "metadata": {},
   "source": [
    "CV WRMSSE= 0.9364420581545198\n",
    "Test WRMSSE= 0.8766403828582977"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e832b",
   "metadata": {},
   "source": [
    "b) CNN-LSTM NN(No Date Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input1=tf.keras.layers.Input(shape=(1,),name='Item_Id')\n",
    "input2=tf.keras.layers.Input(shape=(1,),name='Dept_Id')\n",
    "input3=tf.keras.layers.Input(shape=(1,),name='Cat_Id')\n",
    "input4=tf.keras.layers.Input(shape=(1,),name='Store_Id')\n",
    "input5=tf.keras.layers.Input(shape=(1,),name='State_Id')\n",
    "input6=tf.keras.layers.Input(shape=(1,),name='year')\n",
    "input7=tf.keras.layers.Input(shape=(1,),name='event_name_1')\n",
    "input8=tf.keras.layers.Input(shape=(1,),name='event_name_2')\n",
    "input9=tf.keras.layers.Input(shape=(1,),name='season')\n",
    "input10=tf.keras.layers.Input(shape=(1,23),name='Numerical_features')\n",
    "\n",
    "emb1=tf.keras.layers.Embedding(3050,output_dim=150)(input1)\n",
    "emb1=tf.keras.layers.Conv1D(7,8,padding='same',activation='relu')(emb1)\n",
    "\n",
    "emb2=tf.keras.layers.Embedding(8,output_dim=10)(input2)\n",
    "emb2=tf.keras.layers.Conv1D(3,8,padding='same',activation='sigmoid')(emb2)\n",
    "\n",
    "emb3=tf.keras.layers.Embedding(4,output_dim=10)(input3)\n",
    "emb3=tf.keras.layers.Conv1D(5,8,padding='same',activation='relu')(emb3)\n",
    "\n",
    "emb4=tf.keras.layers.Embedding(11,output_dim=10)(input4)\n",
    "emb4=tf.keras.layers.Conv1D(5,8,padding='same',activation='tanh')(emb4)\n",
    "\n",
    "emb5=tf.keras.layers.Embedding(4,output_dim=10)(input5)\n",
    "emb5=tf.keras.layers.Conv1D(5,8,padding='same',activation='relu')(emb5)\n",
    "\n",
    "emb6=tf.keras.layers.Embedding(2017,output_dim=10)(input6)\n",
    "emb6=tf.keras.layers.Conv1D(5,8,padding='same',activation='sigmoid')(emb6)\n",
    "\n",
    "emb7=tf.keras.layers.Embedding(32,output_dim=10)(input7)\n",
    "emb7=tf.keras.layers.Conv1D(3,8,padding='same',activation='relu')(emb7)\n",
    "\n",
    "emb8=tf.keras.layers.Embedding(6,output_dim=10)(input8)\n",
    "emb8=tf.keras.layers.Conv1D(3,8,padding='same',activation='relu')(emb8)\n",
    "\n",
    "emb9=tf.keras.layers.Embedding(5,output_dim=10)(input9)\n",
    "emb9=tf.keras.layers.Conv1D(3,8,padding='same',activation='relu')(emb9)\n",
    "\n",
    "emb10=tf.keras.layers.Conv1D(3,16,padding='same',activation='tanh')(input10)\n",
    "\n",
    "lstm1=tf.keras.layers.LSTM(50)(emb1)\n",
    "lstm2=tf.keras.layers.LSTM(10)(emb2)\n",
    "lstm3=tf.keras.layers.LSTM(10)(emb3)\n",
    "lstm4=tf.keras.layers.LSTM(10)(emb4)\n",
    "lstm5=tf.keras.layers.LSTM(10)(emb5)\n",
    "lstm6=tf.keras.layers.LSTM(10)(emb6)\n",
    "lstm7=tf.keras.layers.LSTM(10)(emb7)\n",
    "lstm8=tf.keras.layers.LSTM(10)(emb8)\n",
    "lstm9=tf.keras.layers.LSTM(10)(emb9)\n",
    "lstm10=tf.keras.layers.LSTM(32)(emb10)\n",
    "\n",
    "x1=tf.keras.layers.Flatten()(lstm1)\n",
    "x2=tf.keras.layers.Flatten()(lstm2)\n",
    "x3=tf.keras.layers.Flatten()(lstm3)\n",
    "x4=tf.keras.layers.Flatten()(lstm4)\n",
    "x5=tf.keras.layers.Flatten()(lstm5)\n",
    "x6=tf.keras.layers.Flatten()(lstm6)\n",
    "x7=tf.keras.layers.Flatten()(lstm7)\n",
    "x8=tf.keras.layers.Flatten()(lstm8)\n",
    "x9=tf.keras.layers.Flatten()(lstm9)\n",
    "x10=tf.keras.layers.Flatten()(lstm10)\n",
    "\n",
    "\n",
    "x=tf.keras.layers.Concatenate()([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10])\n",
    "x=tf.keras.layers.BatchNormalization()(x)\n",
    "x=tf.keras.layers.Dense(256,activation='sigmoid')(x)\n",
    "x=tf.keras.layers.Dense(128,activation='tanh')(x)\n",
    "x=tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(1)(x)\n",
    "model=tf.keras.Model([input1,input2,input3,input4,input5,input6,input7,input8,input9,input10],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68323ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[train['item_id'].values.reshape(-1,1),train['dept_id'].values.reshape(-1,1),train['cat_id'].values.reshape(-1,1),train['store_id'].values.reshape(-1,1),\\\n",
    "             train['state_id'].values.reshape(-1,1),train['year'].values.reshape(-1,1),train['event_name_1'].values.reshape(-1,1),train['event_name_2'].values.reshape(-1,1),\\\n",
    "             train['season'].values.reshape(-1,1),train[[ 'roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],\\\n",
    "          y=train['sales'],epochs=10,verbose=2,batch_size=5000,\\\n",
    "          validation_data=([cv['item_id'].values.reshape(-1,1),cv['dept_id'].values.reshape(-1,1),cv['cat_id'].values.reshape(-1,1),cv['store_id'].values.reshape(-1,1),\\\n",
    "             cv['state_id'].values.reshape(-1,1),cv['year'].values.reshape(-1,1),cv['event_name_1'].values.reshape(-1,1),cv['event_name_2'].values.reshape(-1,1),\\\n",
    "             cv['season'].values.reshape(-1,1),cv[[ 'roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],cv['sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba65749",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_model_performances(model,train,cv,test,[cv['item_id'].values.reshape(-1,1),cv['dept_id'].values.reshape(-1,1),cv['cat_id'].values.reshape(-1,1),cv['store_id'].values.reshape(-1,1),\\\n",
    "             cv['state_id'].values.reshape(-1,1),cv['year'].values.reshape(-1,1),cv['event_name_1'].values.reshape(-1,1),cv['event_name_2'].values.reshape(-1,1),\\\n",
    "             cv['season'].values.reshape(-1,1),cv[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)],\\\n",
    "                       [test['item_id'].values.reshape(-1,1),test['dept_id'].values.reshape(-1,1),test['cat_id'].values.reshape(-1,1),test['store_id'].values.reshape(-1,1),\\\n",
    "             test['state_id'].values.reshape(-1,1),test['year'].values.reshape(-1,1),test['event_name_1'].values.reshape(-1,1),test['event_name_2'].values.reshape(-1,1),\\\n",
    "             test['season'].values.reshape(-1,1),test[['roll_7_shift_28_mean',\n",
    "       'roll_14_shift_28_mean', 'roll_30_shift_28_mean',\n",
    "       'roll_60_shift_28_mean', 'roll_360_shift_28_mean',\n",
    "       'roll_7_shift_28_std', 'roll_14_shift_28_std', 'roll_30_shift_28_std',\n",
    "       'roll_60_shift_28_std', 'roll_360_shift_28_std','sell_price','direct_lag_28', 'direct_lag_35', 'direct_lag_42', 'direct_lag_49',\n",
    "       'direct_lag_56', 'direct_lag_63', 'direct_lag_70', 'direct_lag_77',\n",
    "       'direct_lag_84', 'direct_lag_91', 'direct_lag_98','direct_ewm']].values.reshape(-1,1,23)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5deee6",
   "metadata": {},
   "source": [
    "CV WRMSSE= 0.9316142875533537\n",
    "Test WRMSSE= 0.9638282313741626"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
